# -*- coding: utf-8 -*-
"""duplicateAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I_CXQ0J4FvYM5clffWSTBH4WFuHDlRei
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize

# Load the Excel file into a pandas DataFrame
df = pd.read_excel('/content/dataset_v2.xlsx')

# Display the first few rows of the DataFrame
display(df)

"""TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

def find_duplicates_tfidf(df, column="clean", threshold=0.90, min_df=2, max_df=0.85):
    texts = df[column].astype(str).tolist()

    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df)
    X = vectorizer.fit_transform(texts)

    sim_matrix = cosine_similarity(X)

    rows = []
    for i in range(len(sim_matrix)):
        for j in range(i + 1, len(sim_matrix)):
            sim = sim_matrix[i, j]
            if sim >= threshold:
                rows.append({
                    "idx1": i,
                    "text1": texts[i],
                    "idx2": j,
                    "text2": texts[j],
                    "similarity": sim
                })

    return pd.DataFrame(rows)

res_tfidf = find_duplicates_tfidf(df, "review", threshold=0.90)

display(res_tfidf)

"""SBERT"""

from sentence_transformers import SentenceTransformer, util
import torch
import pandas as pd

# Modello consigliato
SBERT_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

sbert = SentenceTransformer(SBERT_MODEL)

def find_duplicates_sbert(df, column="clean", threshold=0.85):
    texts = df[column].astype(str).tolist()

    embeddings = sbert.encode(texts, convert_to_tensor=True, normalize_embeddings=True)
    sim_matrix = util.cos_sim(embeddings, embeddings)

    n = len(texts)
    pairs = []

    rows = []
    for i in range(len(sim_matrix)):
        for j in range(i + 1, len(sim_matrix)):
            sim = sim_matrix[i, j]
            if sim >= threshold:
                rows.append({
                    "idx1": i,
                    "text1": texts[i],
                    "idx2": j,
                    "text2": texts[j],
                    "similarity": sim
                })

    return pd.DataFrame(rows)

res_sbert = find_duplicates_sbert(df, "review", threshold=0.85)
display(res_sbert)

"""E5"""

import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F


# ============================================================
# 1) CARICA MODELLO E5
# ============================================================

MODEL_NAME = "intfloat/multilingual-e5-base"

tokenizer_e5 = AutoTokenizer.from_pretrained(MODEL_NAME)
model_e5 = AutoModel.from_pretrained(MODEL_NAME)


# ============================================================
# 2) FUNZIONE PER OTTENERE UN EMBEDDING E5
# ============================================================

def get_e5_embedding(text: str):
    """
    Restituisce embedding E5 normalizzato (L2).
    """
    formatted = "query: " + text

    tokens = tokenizer_e5(
        formatted,
        return_tensors="pt",
        truncation=True,
        padding=True
    )

    with torch.no_grad():
        output = model_e5(**tokens)
        emb = output.last_hidden_state[:, 0, :]  # CLS token
        emb = F.normalize(emb, p=2, dim=1)

    return emb.squeeze().numpy()


# ============================================================
# 3) COSINE SIMILARITY
# ============================================================

def cosine(a, b):
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


# ============================================================
# 4) NEAR-DUPLICATE DETECTION
# ============================================================

def find_duplicates_e5(df, column="clean", threshold=0.90):
    """
    Identifica near-duplicates usando E5 semantic similarity.
    Restituisce una tabella con tutte le coppie duplicate.
    """
    texts = df[column].astype(str).tolist()

    # ---- Step 1: compute embeddings
    embeddings = np.vstack([get_e5_embedding(t) for t in texts])

    # ---- Step 2: compute full similarity matrix
    sim_matrix = embeddings @ embeddings.T  # perché già normalizzati

    # ---- Step 3: estrai coppie con sim ≥ threshold
    rows = []
    n = len(texts)

    for i in range(n):
        for j in range(i + 1, n):
            sim = sim_matrix[i, j]
            if sim >= threshold:
                rows.append({
                    "idx1": i,
                    "text1": texts[i],
                    "idx2": j,
                    "text2": texts[j],
                    "similarity": float(sim)
                })

    return pd.DataFrame(rows)

res_E5   = find_duplicates_e5(df, "review", threshold=0.92)

display(res_E5)

"""

---
TF-IDF CHOOSED SINCE THE OTHERS REDUCE TOO MUCH THE DATASET
"""

# Load the Excel file into a pandas DataFrame
df = pd.read_excel('/content/dataset_v2.xlsx')

# Display the first few rows of the DataFrame
display(df)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

def find_duplicates_tfidf(df, column="clean", threshold=0.90, min_df=2, max_df=0.85):
    texts = df[column].astype(str).tolist()

    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df)
    X = vectorizer.fit_transform(texts)

    sim_matrix = cosine_similarity(X)

    rows = []
    for i in range(len(sim_matrix)):
        for j in range(i + 1, len(sim_matrix)):
            sim = sim_matrix[i, j]
            if sim >= threshold:
                rows.append({
                    "idx1": i,
                    "text1": texts[i],
                    "idx2": j,
                    "text2": texts[j],
                    "similarity": sim
                })

    return pd.DataFrame(rows)

dups = find_duplicates_tfidf(df, column="review", threshold=0.90)

to_drop = dups["idx2"].unique().tolist()

df_no_duplicates = df.drop(index=to_drop).reset_index(drop=True)

display(df_no_duplicates)

display(df)